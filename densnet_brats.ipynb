{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates import *\n",
    "from templates_cls import *\n",
    "import monai.transforms as T\n",
    "from monai.networks.nets import densenet121\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-25 13:27:40,445 - INFO - Verified 'Task01_BrainTumour.tar', md5: 240a19d752f0d9e9101544901065d872.\n",
      "2024-03-25 13:27:40,446 - INFO - File exists: /DATA/NAS/datasets_source/brain/Task01_BrainTumour.tar, skipped downloading.\n",
      "2024-03-25 13:27:40,446 - INFO - Non-empty folder exists in /DATA/NAS/datasets_source/brain/Task01_BrainTumour, skipped extracting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 388/388 [02:17<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-25 13:30:08,779 - INFO - Verified 'Task01_BrainTumour.tar', md5: 240a19d752f0d9e9101544901065d872.\n",
      "2024-03-25 13:30:08,781 - INFO - File exists: /DATA/NAS/datasets_source/brain/Task01_BrainTumour.tar, skipped downloading.\n",
      "2024-03-25 13:30:08,781 - INFO - Non-empty folder exists in /DATA/NAS/datasets_source/brain/Task01_BrainTumour, skipped extracting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 96/96 [00:32<00:00,  2.93it/s]\n"
     ]
    }
   ],
   "source": [
    "channel = 0  # 0 = Flair\n",
    "train_transforms = T.Compose(\n",
    "[\n",
    "        T.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        T.EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        T.Lambdad(keys=[\"image\"], func=lambda x: x[channel, None, :, :, :]),\n",
    "        T.EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        T.Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        T.Spacingd(keys=[\"image\", \"label\"], pixdim=(3.0, 3.0, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        T.CenterSpatialCropd(keys=[\"image\", \"label\"], roi_size=(64, 64, 64)),\n",
    "        T.ScaleIntensityRangePercentilesd(keys=\"image\", lower=0, upper=99.5, b_min=0, b_max=1),\n",
    "        T.RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=(64, 64, 1), random_size=False),\n",
    "        T.Lambdad(keys=[\"image\", \"label\"], func=lambda x: x.squeeze(-1)),\n",
    "        T.CopyItemsd(keys=[\"label\"], times=1, names=[\"slice_label\"]),\n",
    "        T.Lambdad(keys=[\"slice_label\"], func=lambda x: 1.0 if x.sum() > 0 else 0.0),\n",
    "]\n",
    ")\n",
    "\n",
    "dataset_train =  DecathlonDataset(\n",
    "root_dir=\"/DATA/NAS/datasets_source/brain\",\n",
    "task=\"Task01_BrainTumour\",\n",
    "section=\"training\",\n",
    "cache_rate=1.0,  # you may need a few Gb of RAM... Set to 0 otherwise\n",
    "num_workers=4,\n",
    "download=True,  # Set download to True if the dataset hasnt been downloaded yet\n",
    "seed=0,\n",
    "transform=train_transforms,\n",
    ")\n",
    "\n",
    "val_transforms = T.Compose(\n",
    "[\n",
    "        T.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        T.EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        T.Lambdad(keys=[\"image\"], func=lambda x: x[channel, None, :, :, :]),\n",
    "        T.EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        T.Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        T.Spacingd(keys=[\"image\", \"label\"], pixdim=(3.0, 3.0, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        T.CenterSpatialCropd(keys=[\"image\", \"label\"], roi_size=(64, 64, 1)),\n",
    "        T.ScaleIntensityRangePercentilesd(keys=\"image\", lower=0, upper=99.5, b_min=0, b_max=1),\n",
    "        T.Lambdad(keys=[\"image\", \"label\"], func=lambda x: x.squeeze(-1)),\n",
    "        T.CopyItemsd(keys=[\"label\"], times=1, names=[\"slice_label\"]),\n",
    "        T.Lambdad(keys=[\"slice_label\"], func=lambda x: 1 if x.sum() > 0 else 0),\n",
    "]\n",
    ")\n",
    "\n",
    "dataset_val =  DecathlonDataset(\n",
    "root_dir=\"/DATA/NAS/datasets_source/brain\",\n",
    "task=\"Task01_BrainTumour\",\n",
    "section=\"validation\",\n",
    "cache_rate=1.0,  # you may need a few Gb of RAM... Set to 0 otherwise\n",
    "num_workers=4,\n",
    "download=True,  # Set download to True if the dataset hasnt been downloaded yet\n",
    "seed=0,\n",
    "transform=val_transforms,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_loader = DataLoader(dataset_train, batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.6204, Val Loss: 0.5165, Val Acc: 0.9062\n",
      "Epoch 2/100, Train Loss: 0.4991, Val Loss: 0.4646, Val Acc: 0.9167\n",
      "Epoch 3/100, Train Loss: 0.4287, Val Loss: 0.4331, Val Acc: 0.9167\n",
      "Epoch 4/100, Train Loss: 0.3752, Val Loss: 0.4066, Val Acc: 0.9167\n",
      "Epoch 5/100, Train Loss: 0.3308, Val Loss: 0.3858, Val Acc: 0.9167\n",
      "Epoch 6/100, Train Loss: 0.2931, Val Loss: 0.3686, Val Acc: 0.9271\n",
      "Epoch 7/100, Train Loss: 0.2601, Val Loss: 0.3539, Val Acc: 0.9271\n",
      "Epoch 8/100, Train Loss: 0.2310, Val Loss: 0.3423, Val Acc: 0.9271\n",
      "Epoch 9/100, Train Loss: 0.2053, Val Loss: 0.3317, Val Acc: 0.9271\n",
      "Epoch 10/100, Train Loss: 0.1827, Val Loss: 0.3220, Val Acc: 0.9271\n",
      "Epoch 11/100, Train Loss: 0.1629, Val Loss: 0.3140, Val Acc: 0.9271\n",
      "Epoch 12/100, Train Loss: 0.1457, Val Loss: 0.3055, Val Acc: 0.9271\n",
      "Epoch 13/100, Train Loss: 0.1306, Val Loss: 0.2989, Val Acc: 0.9271\n",
      "Epoch 14/100, Train Loss: 0.1177, Val Loss: 0.2923, Val Acc: 0.9271\n",
      "Epoch 15/100, Train Loss: 0.1062, Val Loss: 0.2880, Val Acc: 0.9271\n",
      "Epoch 16/100, Train Loss: 0.0961, Val Loss: 0.2856, Val Acc: 0.9271\n",
      "Epoch 17/100, Train Loss: 0.0872, Val Loss: 0.2840, Val Acc: 0.9271\n",
      "Epoch 18/100, Train Loss: 0.0789, Val Loss: 0.2815, Val Acc: 0.9271\n",
      "Epoch 19/100, Train Loss: 0.0719, Val Loss: 0.2813, Val Acc: 0.9271\n",
      "Epoch 20/100, Train Loss: 0.0665, Val Loss: 0.2801, Val Acc: 0.9271\n",
      "Epoch 21/100, Train Loss: 0.0627, Val Loss: 0.2805, Val Acc: 0.9271\n",
      "Epoch 22/100, Train Loss: 0.0571, Val Loss: 0.2782, Val Acc: 0.9271\n",
      "Epoch 23/100, Train Loss: 0.0532, Val Loss: 0.2792, Val Acc: 0.9271\n",
      "Epoch 24/100, Train Loss: 0.0501, Val Loss: 0.2760, Val Acc: 0.9271\n",
      "Epoch 25/100, Train Loss: 0.0477, Val Loss: 0.2778, Val Acc: 0.9271\n",
      "Epoch 26/100, Train Loss: 0.0439, Val Loss: 0.2779, Val Acc: 0.9271\n",
      "Epoch 27/100, Train Loss: 0.0434, Val Loss: 0.2778, Val Acc: 0.9271\n",
      "Epoch 28/100, Train Loss: 0.0429, Val Loss: 0.2779, Val Acc: 0.9271\n",
      "Epoch 29/100, Train Loss: 0.0424, Val Loss: 0.2778, Val Acc: 0.9271\n",
      "Epoch 30/100, Train Loss: 0.0420, Val Loss: 0.2778, Val Acc: 0.9271\n",
      "Epoch 31/100, Train Loss: 0.0416, Val Loss: 0.2778, Val Acc: 0.9271\n",
      "Epoch 32/100, Train Loss: 0.0413, Val Loss: 0.2778, Val Acc: 0.9271\n",
      "Epoch 33/100, Train Loss: 0.0409, Val Loss: 0.2777, Val Acc: 0.9271\n",
      "Epoch 34/100, Train Loss: 0.0405, Val Loss: 0.2777, Val Acc: 0.9271\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from monai.networks.nets import DenseNet121\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Model setup\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=1).to(device)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
    "\n",
    "# Early Stopping Setup\n",
    "early_stopping_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "max_epochs = 100\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_data in train_loader:\n",
    "        images, labels = batch_data[\"image\"].to(device), batch_data[\"slice_label\"].float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in val_loader:\n",
    "            images, labels = batch_data[\"image\"].to(device), batch_data[\"slice_label\"].float().to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels.unsqueeze(1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.sigmoid(outputs).round() \n",
    "            total_preds.extend(preds.cpu().numpy())\n",
    "            total_labels.extend(labels.cpu().numpy())\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = accuracy_score(total_labels, total_preds)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{max_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state dictionary\n",
    "torch.save(model.state_dict(), \"/home/matan/latent_dae/diffae/resnet/brats_resnet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.49444444444444446\n",
      "F1:  0.9621621621621622\n"
     ]
    }
   ],
   "source": [
    "from monai.networks.nets import DenseNet121\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=1).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"/home/matan/latent_dae/diffae/resnet/brats_resnet.pt\"))\n",
    "model.eval()\n",
    "\n",
    "total_preds = []\n",
    "total_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch_data in val_loader:\n",
    "        images, labels = batch_data['image'].to(device), batch_data['slice_label'].to(device)\n",
    "        labels = (labels > 0)\n",
    "        labels = labels.float()\n",
    "        outputs = model(images)\n",
    "        preds = torch.sigmoid(outputs).round()\n",
    "        total_preds.extend(preds.cpu().numpy())\n",
    "        total_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "auc = roc_auc_score(total_labels, total_preds)\n",
    "f1 = f1_score(total_labels, total_preds)\n",
    "\n",
    "print(\"AUC: \", auc)\n",
    "print(\"F1: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "911026d04c504ad5e0c4c11f5deabc0fa44aa59b5237d77193e02811cb4f84ca"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('myenv': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
