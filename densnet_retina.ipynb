{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates import *\n",
    "from templates_cls import *\n",
    "import monai.transforms as T\n",
    "from monai.utils.misc import first\n",
    "\n",
    "from medmnist.dataset import RetinaMNIST\n",
    "from medmnist.info import DEFAULT_ROOT\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /DATA/NAS/datasets_source/other/retinamnist_128.npz\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_transform = T.Compose([\n",
    "                ToTensor(),\n",
    "                T.RandRotate(range_x=np.pi / 12, prob=0.5, keep_size=True),\n",
    "                T.RandFlip(spatial_axis=[-1, -2], prob=0.5),\n",
    "                T.RandGridDistortion(prob=0.5),\n",
    "                T.RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
    "                T.ScaleIntensity(),\n",
    "            ])\n",
    "dataset_train =  RetinaMNIST(split=\"train\", \n",
    "        transform=train_transform, \n",
    "        download=True, \n",
    "        as_rgb=True, \n",
    "        size=128, \n",
    "        root=\"/DATA/NAS/datasets_source/other\")\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /DATA/NAS/datasets_source/other/retinamnist_128.npz\n"
     ]
    }
   ],
   "source": [
    "val_transform = T.Compose([\n",
    "                ToTensor(),\n",
    "                T.ScaleIntensity(),\n",
    "            ])\n",
    "dataset_val =  RetinaMNIST(split=\"test\", \n",
    "        transform=val_transform, \n",
    "        download=True, \n",
    "        as_rgb=True, \n",
    "        size=128, \n",
    "        root=\"/DATA/NAS/datasets_source/other\")\n",
    "\n",
    "val_loader = DataLoader(dataset_val, batch_size, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 1.2347, Val Loss: 1.2016, Val MAE: 1.1937\n",
      "Epoch 2/100, Train Loss: 1.1259, Val Loss: 1.0779, Val MAE: 1.0699\n",
      "Epoch 3/100, Train Loss: 1.0254, Val Loss: 0.9975, Val MAE: 0.9902\n",
      "Epoch 4/100, Train Loss: 0.9326, Val Loss: 0.9375, Val MAE: 0.9303\n",
      "Epoch 5/100, Train Loss: 0.8594, Val Loss: 0.8966, Val MAE: 0.8896\n",
      "Epoch 6/100, Train Loss: 0.7972, Val Loss: 0.8452, Val MAE: 0.8377\n",
      "Epoch 7/100, Train Loss: 0.7390, Val Loss: 0.8035, Val MAE: 0.7961\n",
      "Epoch 8/100, Train Loss: 0.6845, Val Loss: 0.7871, Val MAE: 0.7788\n",
      "Epoch 9/100, Train Loss: 0.6398, Val Loss: 0.7948, Val MAE: 0.7864\n",
      "Epoch 10/100, Train Loss: 0.5980, Val Loss: 0.7889, Val MAE: 0.7802\n",
      "Epoch 11/100, Train Loss: 0.5605, Val Loss: 0.7944, Val MAE: 0.7860\n",
      "Epoch 12/100, Train Loss: 0.5228, Val Loss: 0.7924, Val MAE: 0.7839\n",
      "Epoch 13/100, Train Loss: 0.4877, Val Loss: 0.7901, Val MAE: 0.7827\n",
      "Epoch 14/100, Train Loss: 0.4511, Val Loss: 0.7815, Val MAE: 0.7736\n",
      "Epoch 15/100, Train Loss: 0.4112, Val Loss: 0.8100, Val MAE: 0.8037\n",
      "Epoch 16/100, Train Loss: 0.3752, Val Loss: 0.8207, Val MAE: 0.8144\n",
      "Epoch 17/100, Train Loss: 0.3379, Val Loss: 0.7972, Val MAE: 0.7868\n",
      "Epoch 18/100, Train Loss: 0.3112, Val Loss: 0.7982, Val MAE: 0.7881\n",
      "Epoch 19/100, Train Loss: 0.2979, Val Loss: 0.8087, Val MAE: 0.8002\n",
      "Epoch 20/100, Train Loss: 0.2811, Val Loss: 0.7979, Val MAE: 0.7892\n",
      "Epoch 21/100, Train Loss: 0.2566, Val Loss: 0.7888, Val MAE: 0.7765\n",
      "Epoch 22/100, Train Loss: 0.2505, Val Loss: 0.8616, Val MAE: 0.8542\n",
      "Epoch 23/100, Train Loss: 0.2292, Val Loss: 0.8463, Val MAE: 0.8354\n",
      "Epoch 24/100, Train Loss: 0.2196, Val Loss: 0.8263, Val MAE: 0.8120\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from monai.networks.nets import DenseNet121\n",
    "from sklearn.metrics import mean_absolute_error, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming train_loader and val_loader are defined\n",
    "\n",
    "# Model setup\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DenseNet121(spatial_dims=2, in_channels=3, out_channels=1).to(device)\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
    "\n",
    "# Early Stopping Setup\n",
    "early_stopping_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "max_epochs = 100\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_data in train_loader:\n",
    "        images, labels = batch_data[0].to(device), batch_data[1].float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in val_loader:\n",
    "            images, labels = batch_data[0].to(device), batch_data[1].float().to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = outputs\n",
    "            total_preds.extend(preds.cpu().numpy())\n",
    "            total_labels.extend(labels.cpu().numpy())\n",
    "    val_loss /= len(val_loader)\n",
    "    val_mae = mean_absolute_error(total_labels, total_preds)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{max_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}')\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state dictionary\n",
    "torch.save(model.state_dict(), \"/home/matan/latent_dae/diffae/resnet/retina_resnet_reg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.7647887\n",
      "F1:  0.3140680188814577\n"
     ]
    }
   ],
   "source": [
    "from monai.networks.nets import DenseNet121\n",
    "from sklearn.metrics import mean_absolute_error, f1_score\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DenseNet121(spatial_dims=2, in_channels=3, out_channels=1).to(device)\n",
    "model.load_state_dict(torch.load(\"/home/matan/latent_dae/diffae/resnet/retina_resnet_reg.pt\"))\n",
    "model.eval()\n",
    "\n",
    "total_preds = []\n",
    "total_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch_data in val_loader:\n",
    "        images, labels = batch_data[0].to(device), batch_data[1].float().to(device)\n",
    "        preds = model(images)\n",
    "        total_preds.extend(preds.cpu().numpy())\n",
    "        total_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "val_mae = mean_absolute_error(total_labels, total_preds)\n",
    "val_f1 = f1_score(total_labels, np.round(total_preds), average=\"macro\")\n",
    "\n",
    "print(\"MAE: \", val_mae)\n",
    "print(\"F1: \", val_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.6357, Val Loss: 0.6162, Val ACC: 0.7333\n",
      "Epoch 2/100, Train Loss: 0.5530, Val Loss: 0.5099, Val ACC: 0.8167\n",
      "Epoch 3/100, Train Loss: 0.5043, Val Loss: 0.4710, Val ACC: 0.8417\n",
      "Epoch 4/100, Train Loss: 0.4691, Val Loss: 0.4511, Val ACC: 0.8500\n",
      "Epoch 5/100, Train Loss: 0.4408, Val Loss: 0.4395, Val ACC: 0.8500\n",
      "Epoch 6/100, Train Loss: 0.4162, Val Loss: 0.4326, Val ACC: 0.8500\n",
      "Epoch 7/100, Train Loss: 0.3935, Val Loss: 0.4293, Val ACC: 0.8500\n",
      "Epoch 8/100, Train Loss: 0.3713, Val Loss: 0.4270, Val ACC: 0.8417\n",
      "Epoch 9/100, Train Loss: 0.3489, Val Loss: 0.4266, Val ACC: 0.8417\n",
      "Epoch 10/100, Train Loss: 0.3254, Val Loss: 0.4268, Val ACC: 0.8417\n",
      "Epoch 11/100, Train Loss: 0.3003, Val Loss: 0.4277, Val ACC: 0.8417\n",
      "Epoch 12/100, Train Loss: 0.2734, Val Loss: 0.4289, Val ACC: 0.8417\n",
      "Epoch 13/100, Train Loss: 0.2444, Val Loss: 0.4316, Val ACC: 0.8417\n",
      "Epoch 14/100, Train Loss: 0.2126, Val Loss: 0.4335, Val ACC: 0.8417\n",
      "Epoch 15/100, Train Loss: 0.1793, Val Loss: 0.4397, Val ACC: 0.8500\n",
      "Epoch 16/100, Train Loss: 0.1462, Val Loss: 0.4474, Val ACC: 0.8250\n",
      "Epoch 17/100, Train Loss: 0.1164, Val Loss: 0.4565, Val ACC: 0.8250\n",
      "Epoch 18/100, Train Loss: 0.0930, Val Loss: 0.4588, Val ACC: 0.8250\n",
      "Epoch 19/100, Train Loss: 0.0753, Val Loss: 0.4619, Val ACC: 0.8333\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from monai.networks.nets import DenseNet121\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Model setup\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DenseNet121(spatial_dims=2, in_channels=3, out_channels=1).to(device)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
    "\n",
    "# Early Stopping Setup\n",
    "early_stopping_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "max_epochs = 100\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_data in train_loader:\n",
    "        images, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "        labels = (labels > 0)\n",
    "        labels = labels.float()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in val_loader:\n",
    "            images, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "            labels = (labels > 0)\n",
    "            labels = labels.float()\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.sigmoid(outputs)\n",
    "            total_preds.extend(preds.cpu().numpy())\n",
    "            total_labels.extend(labels.cpu().numpy())\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = accuracy_score(total_labels, np.round(total_preds))\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{max_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val ACC: {val_acc:.4f}')\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state dictionary\n",
    "torch.save(model.state_dict(), \"/home/matan/latent_dae/diffae/resnet/retina_resnet_cls.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.7932560268538297\n",
      "F1:  0.8268398268398268\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "model = DenseNet121(spatial_dims=2, in_channels=3, out_channels=1).to(device)\n",
    "model.load_state_dict(torch.load(\"/home/matan/latent_dae/diffae/resnet/retina_resnet_cls.pt\"))\n",
    "model.eval()\n",
    "\n",
    "total_preds = []\n",
    "total_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch_data in val_loader:\n",
    "        images, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "        labels = (labels > 0)\n",
    "        labels = labels.float()\n",
    "        outputs = model(images)\n",
    "        preds = torch.sigmoid(outputs).round()\n",
    "        total_preds.extend(preds.cpu().numpy())\n",
    "        total_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "auc = roc_auc_score(total_labels, total_preds)\n",
    "f1 = f1_score(total_labels, total_preds)\n",
    "\n",
    "print(\"AUC: \", auc)\n",
    "print(\"F1: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "911026d04c504ad5e0c4c11f5deabc0fa44aa59b5237d77193e02811cb4f84ca"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('myenv': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
